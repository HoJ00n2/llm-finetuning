{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀프 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1111)\n",
    "\n",
    "batch_size, seq_length, num_channels = 2, 4, 4\n",
    "input_tensor = torch.randn(batch_size, self, num_channels)\n",
    "\n",
    "# 각 헤드의 크기\n",
    "head_size = 16\n",
    "\n",
    "# K,Q,V 변환을 위한 선형 레이어\n",
    "# 왜 num_channels x head_size의 크기를 가지지?\n",
    "key_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "query_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "value_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "\n",
    "# K,Q,V 변환 수행 -> K,Q,V matrix 생성\n",
    "keys = key_transform(input_tensor) # (2,4,4) -> 각 (2,4,16)의 shape로 변경\n",
    "queries = query_transform(input_tensor)\n",
    "values = value_transform(input_tensor)\n",
    "\n",
    "print(values.shape) \n",
    "\n",
    "# Attention 스코어 계산 (QK)\n",
    "attention_score = queries @ keys.transpose(-2, -1) # ?\n",
    "\n",
    "# 하삼각행렬 생성 및 마스킹\n",
    "# 얘도 역시 현재 이전 정보까지만 확인하기 위함?\n",
    "mask_lower_triangle = torch.tril(torch.ones(seq_length, seq_length)) # 문장끼리 계산할거라, 문장 길이만큼만 생성\n",
    "attention_scores = attention_score.masked_fill(mask_lower_triangle == 0, float('-inf')) # 마스킹행렬 0인 부분 -inf로 변환\n",
    "\n",
    "# 소프트맥스를 적용해 확률 정규화\n",
    "normalized_scores = F.softmax(attention_scores, dim = -1)\n",
    "\n",
    "# 최종 출력 계산 (QK * V)\n",
    "output_tensor = normalized_scores @ values\n",
    "\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 스케일링 단순 예시\n",
    "batch_size, seq_length, embedding_dim = 2,4,4\n",
    "\n",
    "k = torch.randn(batch_size, seq_length, embedding_dim)\n",
    "q = torch.randn(batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# 스케일링 적용 x\n",
    "weight = q @ k.transpose(-2,-1)\n",
    "weight.var() # 큰 값 tensor(4.7050)\n",
    "\n",
    "# 스케일링 적용\n",
    "weight = q @ k.transpose(-2,-1) * (embedding_dim ** -0.5) # ebmedding_dim의 제곱근으로 스케일링\n",
    "weight.var() # 작은 값 tensor(0.4508)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 셀프 어텐션에서의 스케일링\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manusal_seed(1111)\n",
    "\n",
    "batch_size, seq_length, num_channels = 2, 4, 4\n",
    "input_tensor = torch.randn(batch_size, seq_length, num_channels)\n",
    "\n",
    "head_size = 16\n",
    "\n",
    "key_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "query_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "value_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "\n",
    "keys = key_transform(input_tensor)\n",
    "queries = query_transform(input_tensor)\n",
    "values = value_transform(input_tensor)\n",
    "\n",
    "# 스케일링 적용한 어텐션 스코어 계산\n",
    "scailing_factor = num_channels ** -0.5 # squared root k dim\n",
    "attention_scores = queries @ keys.transpose(-2,-1) * scailing_factor # (QK)/root(dk)\n",
    "\n",
    "# 하삼각 행렬 마스킹\n",
    "mask = torch.tril(torch.ones(seq_length, seq_length)) # [4,4]\n",
    "attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "# 소프트 맥스를 적용해 정규화\n",
    "normalized_attention = F.softmax(attention_scores, dim = -1)\n",
    "\n",
    "# 최종 출력 계산 (QK)/root(dk) * V\n",
    "output = normalized_attention @ values\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 셀프 어텐션 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        # self.register_buffer의 별명을 \"tril\"로 지정 -> self.tril로 호출하면 아래의 기능을 수행\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size))) # 마스크 필터 생성\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size, sequence_length, embedding_dim = inputs.shape\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        \n",
    "        weights = queries @ keys.transpose(-2,-1) * (embedding_dim ** -0.5)\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\")\n",
    "        )\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        values = self.value(inputs)\n",
    "        outputs = weights @ values\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Head 클래스와 통합한 semiGPT 클래스 수정\n",
    "\n",
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_length, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.attention_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_length)\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        batch, sequence = inputs.shape\n",
    "\n",
    "        token_embed = self.token_embedding_table(inputs) # 여긴왜 입력이 1개만?\n",
    "        pos_embed = self.position_embedding_table(\n",
    "            torch.arrange(sequence, device=device) # ?\n",
    "            )\n",
    "        x = token_embed + pos_embed # 토큰 정보 + 위치 정보\n",
    "        x = self.attention_head(x) # x만큼 셀프 어텐션 dim 생성 (각 단어를 표현할 dim)\n",
    "        logits = self.lm_head(x) # vocab_length 만큼 dim 조절\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, sequence, embed_size = logits.shape\n",
    "            logits = logits.view(batch * sequence, embed_size)\n",
    "            targets = targets.view(batch * sequence)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    # 다음 단어 생성하는 함수\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cond = inputs[:, -block_size:] # 이것의 의미?\n",
    "            logits, loss = self(inputs_cond) # self() 의 의미?\n",
    "            logits = logits[:, -1, :] # 마지막 sequence에 대해서만 판단\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1) \n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
