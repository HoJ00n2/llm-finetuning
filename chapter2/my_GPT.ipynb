{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set((ko_text))))\n",
    "ko_vocab_size = len(ko_chars)\n",
    "print(\"총 글자 수 :\", ko_vocab_size)\n",
    "\n",
    "character_to_ids = {char:i for i, char in enumerate(ko_chars)}\n",
    "ids_to_character = {i:char for i, char in enumerate(ko_chars)}\n",
    "token_encode = lambda s:[character_to_ids[c] for c in s]\n",
    "token_decode = lambda l: \"\".join([ids_to_character[i] for i in l])\n",
    "print(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\"))\n",
    "print(token_decode(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "fron torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        # 임베딩은 각 단어를 고유한 숫자 벡터로 변환하는 역할\n",
    "        # 텍스트를 숫자로 만듦으로써 컴퓨터가 이해하고 처리할 수 있게 변환\n",
    "        # nn.Embedding() 의 첫 번째 vocab_length는 총 단어 수를 의미\n",
    "        # 두 번재 vocab_length는 각 단어를 표현할 벡터의 크기를 나타냄 (= 각 단어에 대해 2701개의 feature로 표현)\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length) # (vocab_length, vocab_length)의 임베딩 생성\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.embedding_token_table(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model = semiGPT(ko_vocab_size) # ko_vocab_size = 2701\n",
    "output = model(example_x, example_y) \n",
    "print(output.shape) # torch.Size([4, 8, 2701]) # (배치, 블록, 중복되지 않은 총 글자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Loss 추가\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # 입력에 대한 확률분포 구하기\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        # 입력 확률 분포와 레이블 확률 분포 사이 손실함수 구하기\n",
    "        loss = F.cross_entropy(logits, targets) # shape 에러 발생!\n",
    "        return logits, loss\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "output, loss = model(example_x, example_y)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# shape 추가한 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs, inputs)\n",
    "        \n",
    "        # shape 통일\n",
    "        # batch, seq_length, vocab_length = logits.shape\n",
    "        # logits = logits.view(batch * seq_length, vocab_length)\n",
    "        logits = logits.view(32, -1)\n",
    "        \n",
    "        # batch, seq_length = targets.shape\n",
    "        # targets = targets.view(batch * seq_length)\n",
    "        targets = targets.view(32)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# generate 메서드 추가\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class SemiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        self.embedding_token_table(vocab_length, vocab_length)\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs, inputs)\n",
    "        logits = logits.view(32, -1)\n",
    "\n",
    "        targets = targets.view(32)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            print(logits)\n",
    "\n",
    "            probs = F.softmax(logits, dims=1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1) # 가장 높은 확률로 나온 다음 단어 예측\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1) # 기존 inputs과 생성된 new_inputs 추가하여 업뎃\n",
    "        return inputs\n",
    "\n",
    "model = SemiGPT(ko_vocab_size)\n",
    "logits, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "# token_decode는 그저 idx를 매핑된 글자로 바꿔주는 함수\n",
    "# 여기서 예측 idx 생성은 generate함수가 수행\n",
    "token_decode(model.generate(torch.zeros((1,1),\n",
    "                            dtype=torch.long),\n",
    "                            max_new_tokens=10[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = 1e-2\n",
    "model = semiGPT(ko_vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "for steps in tqdm(range(10000)):\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    # 옵티마이저 초기화 \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # 역전파 계산 \n",
    "    loss.backward()\n",
    "    # 가중치 업데이트 \n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 GPU에 전달하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "# 기존에 있던 batch_function에 gpu에 올리는 전처리 부분 추가\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0 :\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f'step : {step}, train loss : {losses[\"train\"]:.4f}, val loss : {losses[\"eval\"]:.4f}')\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
